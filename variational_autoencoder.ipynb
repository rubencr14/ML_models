{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIATIONAL AUTOENCODER EXAMPLE (TENSORFLOW)\n",
    "In this example we are going to use the MNIST dataset downloaded directly from tensorflow.examples.\n",
    "First, we specify the different variables:\n",
    "batch_size: number of instances used in the minibatch gradient descent approximation\n",
    "X_in: Defining a placeholder for saving the images used in the training examples. The shape is the shape of the tensor\n",
    "to be fed. None means that it will be the size of the batch. \n",
    "Y: This placeholder will hold the images reconstructed by the decoder part of the autoencoder\n",
    "Y_flat: Y reshaped in order to be used when computing losses\n",
    "keep_prob: This is used as a placeholder when using the dropout regularization technique\n",
    "reshaped_dim: dimension of the decoder when sarting the transpose convolutional2D layers after the dense layers\n",
    "architecture\n",
    "inputs_decoder: The dimension of the inputs in the decoder phase\n",
    "The leaky_relu is defined since tensorflow does not have a predefined one (since 1.4.0 release tf.nn.leaky_relu can be used)\n",
    "\n",
    "The encoder if the first part of the architecture. In this phase, the aim is to find a lower dimensional representation of the images until obtaining the latent variables Z. Firstly, the images are reshaped according to the tensor dimensions:\n",
    "[batch_size, height, width, channels]. -1 in batch_size means using the batch size defined when splitting the dataset\n",
    "into batches. RGB for example has 3 channels whereas white and black images have 1 as channels.\n",
    "The encoder has basically convolutional layers since we are treating with images. However, other types of architectures could be employed instead. \n",
    "The mean and standard deviation necessary for the reparametrization trick will be computed using two dense layers which\n",
    "will be optimized for computing them via the gradients.\n",
    "The decoder can sample from Q(Z|X) distribution in order to obtain the images back to their real form. Nonetheless, since sampling is stochastic we cannot use backpropagation! To solve this we can apply: z = mean + deviation * epsilon (* meaning elementwise or Hadamard multiplication). This way, all the randomness is saved in the epsilon variables and now\n",
    "we can use gradients in order to optimize the theta and phi parameters. This is called the reparametrization trick.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST\", one_hot=False)\n",
    "print(\"minist \", mnist.test.labels); exit()\n",
    "tf.reset_default_graph()\n",
    "batch_size = 64\n",
    "\n",
    "X_in = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"X\")\n",
    "Y = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"Y\")\n",
    "Y_flat = tf.reshape(Y, shape=[-1, 28 * 28])\n",
    "keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name=\"keep_prob\") #Used for dropout layers\n",
    "number_of_latent_variables = 20\n",
    "dec_in_channels = 1\n",
    "reshaped_dim = [-1, 7, 7, dec_in_channels]\n",
    "inputs_decoder = 49 * dec_in_channels / 2\n",
    "\n",
    "def leaky_relu(x, alpha=0.3):\n",
    "    return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "\n",
    "def encoder(X_in, keep_prob):\n",
    "    \n",
    "    with tf.variable_scope(\"encoder\", reuse=None):\n",
    "        X_reshaped = tf.reshape(X_in, shape=[-1, 28, 28, 1]) #reshape where -1 means the batch_size, \n",
    "                                                            #28 the height, 28 the width and 1 the number of channels!\n",
    "        X = tf.layers.conv2d(X_reshaped, filters=64, kernel_size=4, strides=2, padding=\"same\", activation=leaky_relu)\n",
    "        X = tf.nn.dropout(X, keep_prob)\n",
    "        X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=2, padding=\"same\", activation=leaky_relu)\n",
    "        \n",
    "        X = tf.nn.dropout(X, keep_prob)\n",
    "        X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=1, \n",
    "                             padding=\"same\", activation=leaky_relu)\n",
    "        X = tf.nn.dropout(X, keep_prob)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "        mean = tf.layers.dense(X, units=number_of_latent_variables)\n",
    "        sd = 0.5 * tf.layers.dense(X, units=number_of_latent_variables)\n",
    "        epsilon = tf.random_normal([tf.shape(X)[0], number_of_latent_variables])\n",
    "        z = mean + tf.multiply(epsilon, tf.exp(sd)) #tf.multiply is elementwise or Hadamard multiplication!\n",
    "        \n",
    "        return z, mean, sd\n",
    "    \n",
    "def decoder(sampled_z, keep_prob):\n",
    "    \n",
    "    with tf.variable_scope(\"decoder\", reuse=None):\n",
    "        X = tf.layers.dense(sampled_z, units=inputs_decoder, activation=leaky_relu)\n",
    "        X = tf.layers.dense(X, units=inputs_decoder * 2 + 1, activation=leaky_relu)\n",
    "        X = tf.reshape(X, reshaped_dim)\n",
    "        X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=2, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "        X = tf.nn.dropout(X, keep_prob)\n",
    "        X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "        X = tf.nn.dropout(X, keep_prob)\n",
    "        X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                             padding=\"same\", activation=tf.nn.relu)\n",
    "        X = tf.contrib.layers.flatten(X)\n",
    "        X = tf.layers.dense(X, units=28*28, activation=tf.nn.sigmoid)\n",
    "        img = tf.reshape(X, shape=[-1, 28, 28])\n",
    "        \n",
    "        return img\n",
    "\n",
    "    \n",
    "sampled, mn, sd = encoder(X_in, keep_prob)\n",
    "dec = decoder(sampled, keep_prob)\n",
    "\n",
    "unreshaped = tf.reshape(dec, [-1, 28 * 28])\n",
    "img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, Y_flat), 1) #Log-likelihood \n",
    "latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * sd - tf.square(mn) - tf.exp(2.0 * sd), 1) #Kullback-Leibler divergence\n",
    "loss = tf.reduce_mean(img_loss + latent_loss)\n",
    "optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "iterations = 3000\n",
    "\n",
    "for i in range(iterations):\n",
    "    \n",
    "    batch = [np.reshape(b, [28, 28]) for b in mnist.train.next_batch(batch_size=batch_size)[0]]\n",
    "    #labels = mnist.train.labels\n",
    "    labels = mnist.train.next_batch(batch_size=batch_size)[1]\n",
    "    sess.run(optimizer, feed_dict={X_in: batch, Y: batch, keep_prob: 0.8})\n",
    "    if not i % 200:\n",
    "        ls, d, i_ls, d_ls, mu, sigm = sess.run([loss, dec, img_loss, latent_loss, mn, sd], feed_dict={X_in: batch, Y: batch, keep_prob: 1.0})\n",
    "        plt.imshow(np.reshape(batch[0], [28, 28]), cmap=\"gray\")\n",
    "        plt.show()\n",
    "        plt.imshow(d[0], cmap=\"gray\")\n",
    "        plt.show()\n",
    "        if number_of_latent_variables == 20:\n",
    "            coords = sess.run(sampled, feed_dict={X_in: batch, Y: batch, keep_prob: 0.8})\n",
    "            colormap = ListedColormap(sns.color_palette(sns.hls_palette(10, l=.45, s=.8)).as_hex())\n",
    "            pca = PCA(n_components=2)\n",
    "            components = pca.fit_transform(coords)\n",
    "            plt.scatter(components[:,0], components[:,1], c=labels, cmap=colormap);\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerating new data from distribution Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randoms = [np.random.normal(0, 1, number_of_latent_variables) for _ in range(20)]\n",
    "imgs = sess.run(dec, feed_dict={sampled: randoms, keep_prob: 1.0})\n",
    "imgs = [np.reshape(imgs[i], [28, 28]) for i in range(len(imgs))]\n",
    "\n",
    "for img in imgs:\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
