{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:40px; font-weight:bold; margin:20px; margin-bottom:100px; text-align: justify;text-shadow: 1px 1px 1px #919191,\n",
    "        1px 2px 1px #919191,\n",
    "        1px 3px 1px #919191,\n",
    "        1px 4px 1px #919191,\n",
    "        1px 5px 1px #919191,\n",
    "        1px 6px 1px #919191,\n",
    "        1px 7px 1px #919191,\n",
    "        1px 8px 1px #919191,\n",
    "        1px 9px 1px #919191,\n",
    "        1px 10px 1px #919191,\n",
    "    1px 18px 6px rgba(16,16,16,0.4),\n",
    "    1px 22px 10px rgba(16,16,16,0.2),\n",
    "    1px 25px 35px rgba(16,16,16,0.2),\n",
    "    1px 30px 60px rgba(16,16,16,0.4)\">Long Short Term Memory Recurrent Networks</div>\n",
    "\n",
    "<div style=\"font-style: italic; font-weight: bold; font-size:35px; text-align:center; font-family: Garamond\">by Rubén Cañadas Rodríguez</div>\n",
    "\n",
    "<div style=\"font-size: 30px; margin: 20px; margin-bottom: 40px; margin-left: 0px; line-height: 40pt\">\n",
    "\n",
    "<div style=\"font-size: 30px; font-family: Garamond; font-weight: bold; margin: 30px; margin-left: 0px; margin-bottom: 10px; \">Contents</div>\n",
    "<ol>\n",
    "<li>Introduction</li>\n",
    "<li>Recurrent Neural Networks (RNNs)</li>\n",
    "<li>Long Short Term Memory (LSTM)</li> \n",
    "<li>Text generation</li> \n",
    "<li>Coding</li> \n",
    "</ol>\n",
    "</div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Introduction </div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Recurrent Neural Networks (RNNs) </div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Long Short Term Memory (LSTM) </div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Text generation </div>\n",
    "<div style=\"font-size: 30px; font-weight: bold; margin-bottom: 20px; margin-top: 30px\"> Coding </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 40px; font-weight: bold; text-align: left\">Import packages and modules</div>\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re #regular expressions for treating text\n",
    "import numpy as np\n",
    "from nltk import word_tokenize #Natural Language Processing package\n",
    "from nltk import word_tokenize\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Input, Activation, Dense, Dropout\n",
    "from keras.layers import LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation(object):\n",
    "    \n",
    "    def __init__(self, max_lenght, step):\n",
    "        \n",
    "        self._csv = \"QUOTE.csv\"\n",
    "        self.__df = pd.read_csv(self._csv)\n",
    "        self.__quotes = list(self.__df.quote + \"\\n\") #adding a break at the end of each quote!\n",
    "        self.__chars_to_remove = ['#', '$', '%', '(', ')', '=', ';' ,':',  '*', '+', '£' , '—','’']\n",
    "        self.__cleaned_quotes = []\n",
    "        self._chars = None\n",
    "        self._char_indices = None\n",
    "        self._indices_char = None\n",
    "        self._sentences = []\n",
    "        self._next_chars = []\n",
    "        self._max_length = max_lenght\n",
    "        self._step = step\n",
    "        \n",
    "    def __str__(self):  \n",
    "        return \"{}\".format(self.__quotes)\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.__quotes)\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.__quotes[index]\n",
    "    \n",
    "    @property\n",
    "    def chars_to_remove(self):\n",
    "        return self.__chars_to_remove #Getter method for obtaining the default chars to remove\n",
    "    \n",
    "    @chars_to_remove.setter\n",
    "    def chars_to_remove(self, list_of_chars):\n",
    "        self.__chars_to_remove = list_of_chars #Setter method for changing the chars to remove from sentences\n",
    "        \n",
    "        \n",
    "    def __remove_unused_chars(self):\n",
    "        \n",
    "        \"\"\"\n",
    "        This method removes caracthers that we do not want to include in our model\n",
    "        saved in self.__chars_to_remove attribute. Also we remove more than two spaces\n",
    "        in ours sentences. This method appends the results to cleaned_chars variable\n",
    "        \"\"\"\n",
    "        \n",
    "        for quote in self.__quotes:\n",
    "            for char in self.__chars_to_remove:\n",
    "                new_quote = quote.replace(char, ' ')\n",
    "                pattern = re.compiler(r'\\s{2,}') #regular expression for replacing more than two white spaces\n",
    "                quote = re.sub(pattern, ' ', quote)\n",
    "                self.__cleaned_quotes.append(quote)\n",
    "                  \n",
    "    def __obtain_char_indices(self):\n",
    "        \n",
    "        self.__remove_unused_chars() #Creatin cleaned_chars variables that was initialized as empty list\n",
    "        text = ' '.join(self.__cleaned_chars)\n",
    "        self._chars = sorted(list(set(text))) #We extract all the characters (not repeated ) that are present in the sentences\n",
    "        self._char_indices = dict((c, i) for i, c in enumerate(chars)) #To each character we assign a number\n",
    "        self._indices_char = dict((i, c) for i, c in enumerate(chars)) #The contrary, to each number we asssign a character\n",
    "        \n",
    "    \n",
    "    def _generate_sentences(self):\n",
    "        \n",
    "        for quote in self.__cleaned_quotes:\n",
    "            for i in range(0, len(quote) - self._max_length, self._step):\n",
    "                sentences.append(quote[i: i + self._max_length]) #sentence of lenght maxlen\n",
    "                next_chars.append(quote[i + self._max_length]) #next char after sentence of max lenght\n",
    "            self._sentences.append(quote[-self._max_length:])\n",
    "            self._next_chars.append(quote[-1])\n",
    "        self._sentences = self._sentences[:100] #Optional to reduce time consumption we limit the number of sentences\n",
    "\n",
    "    \n",
    "    def _vectorization(self):\n",
    "        \n",
    "        if not self._sentences:\n",
    "            raise ValueError(\"_generate_sentences method has to be applied before vectorizing!! Otherwise execute generate_and_vectorize \")\n",
    "        \n",
    "        x = np.zeros((len(self._sentences), self._max_length, len(self._chars)), dtype=np.bool) #Three dimensional tensor: for each sentence and \n",
    "        #each char of the sentence we assign an index corresponding to a particular char\n",
    "        y = np.zeros((len(self._sentences), len(self._chars)), dtype=np.bool) #Two dimensional tensor, for each sentence (of maxlen) we assign a next\n",
    "        #char that the LSTM will have to guess given X. \n",
    "        for i, sentence in enumerate(self._sentences):\n",
    "            for t, char in enumerate(sentence): \n",
    "                x[i, t, self._char_indices[char]] = 1 # Tensor[sentences, chars, indices_char]\n",
    "            y[i, self._char_indices[self._next_chars[i]]] = 1 # Tensor [sentence, next_char_in_sentence]\n",
    "            \n",
    "        return x,y\n",
    "        \n",
    "    def generate_and_vectorize(self):\n",
    "        \n",
    "        self._generate_sentences()\n",
    "        return self._vectorization() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:100%; margin: 20px; margin-left:-300px\">\n",
    "  <tr>\n",
    "    <th>Predictors (X train)</th>\n",
    "    <th>Labels (Y train)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they</td>\n",
    "    <td>are</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they are</td>\n",
    "    <td>learning</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>they are learning</td>\n",
    "    <td>artificial</td>\n",
    "  </tr>\n",
    "      <tr>\n",
    "    <td>they are learning artificial</td>\n",
    "    <td>inteligence</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainLSTM(DataPreparation):\n",
    "\n",
    "    def __init__(self, epochs=15, batch_size=1000, max_lenght=15, step=6):\n",
    "        super(TrainLSTM, self).__init__(max_lenght, step)\n",
    "        self.__epochs = epochs\n",
    "        self.__batch_size = batch_size\n",
    "        self.__model = Sequential()\n",
    "    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"Model parameters: \\n batch size: {}\\n number of epochs: {}\".format(self.__batch_size, self.__epochs)\n",
    "    \n",
    "    @property    \n",
    "    def num_epochs(self):\n",
    "        return self.__epochs\n",
    "    \n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        return self.__batch_size\n",
    "        \n",
    "    def model(self):\n",
    "\n",
    "        self.__model.add(Bidirectional(LSTM(256, return_sequences= True, \n",
    "                                     input_shape=(self._max_length, len(self._chars))), name = 'bidirectional'))\n",
    "        self.__model.add(Dropout(0.1, name = 'dropout_bidirectional_lstm'))\n",
    "        self.__model.add(LSTM(64, input_shape=(self._max_length, len(self._chars)), name = 'lstm'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_lstm'))\n",
    "        self.__model.add(Dense(15 * len(self._chars), name = 'first_dense'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_first_dense'))\n",
    "        self.__model.add(Dense(5 * len(self._chars), name = 'second_dense'))\n",
    "        self.__model.add(Dropout(0.1,  name = 'drop_out_second_dense'))\n",
    "        self.__model.add(Dense(len(self._chars), name = 'last_dense'))\n",
    "        self.__model.add(Activation('softmax', name = 'activation'))\n",
    "        self.__model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "    def train(self):\n",
    "        \n",
    "        model.fit([x], y, batch_size=self.__batch_size, epochs=self.__epochs)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
