{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATIVE ADVERSARIAL NETWORK (GAN) PYTORCH\n",
    "In this tutorial a GAN is implemented using Pytorch. A GAN basically uses a decoder in order to generate images, which at the beginning will make no sense since the latent variables are sampled from random noise. Then there is a neural network that acts as a discriminator which function is basically to identify real images and discard those which are fake. The generator and the discriminator will start a challenge in a way that the generator will try to fool the discriminator generating images that it cannot predict wether they are real or fake. This can be done thanks to the loss function:\n",
    "The discriminator loss function will have to parts, the error comming from real images which will try to minimize the difference between the predicted targets and a tensor with all ones (all ones mean that they are all real), and the error comming from the generator (fake images) which will try to minimize the difference between a tensor with all zeros (all zeros mean they are fake) and the images generated with the generator which should be considered fake by the discriminator. \n",
    "The generator loss function has to minimize the difference between the output of the discriminator when it has been fed with fake that and a tensor of all ones (meaning all real). This way the minmax game competition starts!\n",
    "In all the pictures there are some transformations applied before they are fed to the neural net (data preprocessing). Other types of preprocessing functions can be pre-defined such as flippers or zoomers in order to perform data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data \n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\" PARAMETERS \"\"\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "PATH_IMAGES = \"\" #The training images directory\n",
    "PATH = \"\"  #path to save the images generated\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"using device: \", device)\n",
    "NUMBER_OF_EPOCHS = 100\n",
    "\n",
    "def preprocess():\n",
    "\n",
    "    #Image transformations (applying different transformations)\n",
    "    transform = transforms.Compose([transforms.Scale(IMAGE_SIZE),\n",
    "    transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)),])\n",
    "\n",
    "    # Loading the dataset\n",
    "    #dataset = dset.CIFAR10(root=path, download = True, transform = transform)\n",
    "    dataset = dset.ImageFolder(root=PATH_IMAGES, transform=transform)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers = 2)\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "        \n",
    "class DiscriminatorNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__() #Initializing superclass\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            #In pytorch out_channels equals number of filters applied\n",
    "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=4, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(in_channels=512, out_channels=1, kernel_size=4, stride=1, padding=0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.model(input)\n",
    "        return output.view(-1) #View method reshapes!\n",
    "    \n",
    "    \n",
    "    \n",
    "class GeneratorNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=100, out_channels=512, kernel_size=4, stride=1, padding=0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh() #The output is squased from -1 to 1 using the parabolic tangent function\n",
    "\n",
    "            )\n",
    "        \n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.model(input)\n",
    "        return output\n",
    "    \n",
    "def optimizer():\n",
    "    \n",
    "    discriminator = DiscriminatorNet()\n",
    "    discriminator.cuda()\n",
    "    generator = GeneratorNet()\n",
    "    generator.cuda()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    \n",
    "    return discriminator, generator, criterion, optimizer_d, optimizer_g\n",
    "\n",
    "def train():\n",
    "    \n",
    "    discriminator, generator, criterion, optimizer_d, optimizer_g = optimizer()\n",
    "    dataloader = preprocess()\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "\n",
    "        for i, data in enumerate(dataloader,0):\n",
    "\n",
    "            #Now we train the discrimnator which are the real data and the classifier label for\n",
    "            #real data is one, this is why the target is built using a torch.ones which builds a matrix\n",
    "            #in which all the values equals one, and of course the size of the squared-matrix is the input size!\n",
    "\n",
    "            discriminator.zero_grad() #Gradients set to zero!\n",
    "            real, _ = data\n",
    "            inp = Variable(real)\n",
    "            target = Variable(torch.ones(inp.size()[0]))\n",
    "            output = discriminator(inp.cuda())\n",
    "            errD_real = criterion(output.cuda(), target.cuda())\n",
    "\n",
    "\n",
    "            # NOw we train the discriminator with the fake data which labels are 0 and thus, we use the torch.zeros function!\n",
    "\n",
    "            noise = Variable(torch.randn(inp.size()[0], 100, 1, 1))\n",
    "            print(\"latent variable \", noise)\n",
    "            fake = generator(noise.cuda())\n",
    "            target = Variable(torch.zeros(inp.size()[0]))\n",
    "            output = discriminator(fake.detach()) #Using descriminator with the fake that comes from the generator\n",
    "            errD_fake = criterion(output.cuda(), target.cuda()) #Minimizing the different between all zeros (False) \n",
    "            #and the images produced by the generator that should be all zero as well.\n",
    "\n",
    "            errD = errD_real + errD_fake\n",
    "            errD.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "\n",
    "            #NOW THE GENERATOR IS TRAINED!!\n",
    "\n",
    "            generator.zero_grad()\n",
    "            target = Variable(torch.ones(inp.size()[0]))\n",
    "            output = discriminator(fake.cuda())\n",
    "            errG = criterion(output.cuda(), target.cuda()) #Here we want the discriminator to think that the fake are ones\n",
    "            #since ones mean not fake!\n",
    "            errG.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            #print(\"loss_g {} loss_d {}\".format(errG.data, errD.data))\n",
    "            print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 50, i, len(dataloader), errD.data, errG.data))\n",
    "            if i % 100 == 0:\n",
    "                vutils.save_image(real, '%s/real_samples.png' % PATH, normalize = True)\n",
    "                fake = generator(noise.cuda())\n",
    "                vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (PATH, epoch), normalize = True)\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "                        \n",
    "                        \n",
    "                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
