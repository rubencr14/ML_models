{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIATIONAL AUTOENCODER EXAMPLE (TENSORFLOW)\n",
    "In this example we are going to use the MNIST dataset downloaded directly from tensorflow.examples.\n",
    "First, we specify the different variables:\n",
    "batch_size: number of instances used in the minibatch gradient descent approximation\n",
    "X_in: Defining a placeholder for saving the images used in the training examples. The shape is the shape of the tensor\n",
    "to be fed. None means that it will be the size of the batch. \n",
    "Y: This placeholder will hold the images reconstructed by the decoder part of the autoencoder\n",
    "Y_flat: Y reshaped in order to be used when computing losses\n",
    "keep_prob: This is used as a placeholder when using the dropout regularization technique\n",
    "reshaped_dim: dimension of the decoder when sarting the transpose convolutional2D layers after the dense layers\n",
    "architecture\n",
    "inputs_decoder: The dimension of the inputs in the decoder phase\n",
    "The leaky_relu is defined since tensorflow does not have a predefined one (since 1.4.0 release tf.nn.leaky_relu can be used)\n",
    "\n",
    "The encoder if the first part of the architecture. In this phase, the aim is to find a lower dimensional representation of the images until obtaining the latent variables Z. Firstly, the images are reshaped according to the tensor dimensions:\n",
    "[batch_size, height, width, channels]. -1 in batch_size means using the batch size defined when splitting the dataset\n",
    "into batches. RGB for example has 3 channels whereas white and black images have 1 as channels.\n",
    "The encoder has basically convolutional layers since we are treating with images. However, other types of architectures could be employed instead. \n",
    "The mean and standard deviation necessary for the reparametrization trick will be computed using two dense layers which\n",
    "will be optimized for computing them via the gradients.\n",
    "The decoder can sample from Q(Z|X) distribution in order to obtain the images back to their real form. Nonetheless, since sampling is stochastic we cannot use backpropagation! To solve this we can apply: z = mean + deviation * epsilon (* meaning elementwise or Hadamard multiplication). This way, all the randomness is saved in the epsilon variables and now\n",
    "we can use gradients in order to optimize the theta and phi parameters. This is called the reparametrization trick.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from functools import wraps\n",
    "import os, sys, time\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decorators\n",
    "\n",
    "This section include some interesting decorators that will be used in the autoencoder architecture.\n",
    "Basically decorators are methods that modify the functionality of the function they decorate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decorators(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def tensorboad_graph(path=os.getcwd()):\n",
    "        \n",
    "        \"\"\"\n",
    "        This decorator allows to create the graph in tensorboard.\n",
    "        run this command on the terminal in the directory \"path\": python -m tensorboard.main --logdir=.\n",
    "        and then enter the following URL: http://localhost:6006/\n",
    "        \n",
    "        \"\"\"\n",
    "        def mydecorator(func):\n",
    "            @wraps(func)\n",
    "            def mywrapper(self, *args, **kwargs):\n",
    "                func(self, *args, **kwargs)\n",
    "                with tf.Session() as sess:\n",
    "                    writer = tf.summary.FileWriter(os.path.join(path, \"graphs\"), sess.graph)\n",
    "                    msg = \\\n",
    "                    \"\"\"\n",
    "                    run this command on the terminal in the directory \"path\": python -m tensorboard.main --logdir=.\n",
    "                    and then enter the following URL: http://localhost:6006/\n",
    "                    \"\"\"\n",
    "                    print(\"INSTRUCTIONS FOR TENSFORBOARD: \\n {}\".format(msg))\n",
    "            return mywrapper\n",
    "        return mydecorator\n",
    "    \n",
    "    @staticmethod\n",
    "    def pca_latent(n_components=2, plot=True):\n",
    "        \n",
    "        \"\"\"\n",
    "        This decorator will be useful in order to create PCA\n",
    "        representation of the latent variables and to plot it\n",
    "        \n",
    "        \"\"\"\n",
    "        def mydecorator(func):\n",
    "            @wraps(func)\n",
    "            def mywrapper(self, *args, **kwargs):\n",
    "                func(self, *args, **kwargs)\n",
    "                #Implement PCA of latent variables and plotting\n",
    "            return mywrapper\n",
    "        return mydecorator\n",
    "    \n",
    "    @staticmethod\n",
    "    def timing(func):\n",
    "        \n",
    "        \"\"\"\n",
    "        This decorator allows to know how much time is the execution time of the\n",
    "        decoratedd function/method in seconds\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        @wraps(func)\n",
    "        def mywrapper(self, *args, **kwargs):\n",
    "            start = time.time()\n",
    "            return_variable = func(self, *args, **kwargs)\n",
    "            end = time.time()\n",
    "            print(\"Execution time of method {}: {} seconds\".format(func.__name__, round(end-start, 3)))\n",
    "            return return_variable\n",
    "        return mywrapper\n",
    "            \n",
    "            \n",
    "    \n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareData(object):\n",
    "\n",
    "    def __init__(self, batch_size, latent_variables, dec_in_channels):\n",
    "        \n",
    "        self._mnist =  input_data.read_data_sets(\"MNIST\", one_hot=False)\n",
    "        self._X_in = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"X\")\n",
    "        self._Y = tf.placeholder(dtype=tf.float32, shape=[None, 28, 28], name=\"Y\")\n",
    "        self._Y_flat = tf.reshape(self._Y, shape=[-1, 28 * 28])\n",
    "        self._keep_prob = tf.placeholder(dtype=tf.float32, shape=(), name=\"keep_prob\") #Used for dropout layers\n",
    "        self._number_of_latent_variables = latent_variables\n",
    "        self._batch_size = batch_size\n",
    "        self._dec_in_channels = dec_in_channels\n",
    "        self._reshaped_dim = [-1, 7, 7, self._dec_in_channels]\n",
    "        self._inputs_decoder = 49\n",
    "        \n",
    "    def __str__(self):\n",
    "        \n",
    "        return \"variables: \\n latent variables: {} \\n batch size: {}\".format(self._number_of_latent_variables, \n",
    "                                                                             self.batch_size)\n",
    "    \n",
    "class AutoencoderArchitecture(PrepareData):\n",
    "    \n",
    "    def __init__(self, batch_size, latent_variables, dec_in_channels):\n",
    "        super(AutoencoderArchitecture, self).__init__(batch_size, latent_variables, dec_in_channels)\n",
    "        \n",
    "    def leaky_relu(self, x, alpha=0.3):\n",
    "        return tf.maximum(x, tf.multiply(x, alpha))\n",
    "\n",
    "    def _encoder(self):\n",
    "        with tf.variable_scope(\"encoder\", reuse=None):\n",
    "            X_reshaped = tf.reshape(self._X_in, shape=[-1, 28, 28, 1]) #reshape where -1 means the batch_size, \n",
    "                                                                #28 the height, 28 the width and 1 the number of channels!\n",
    "            X = tf.layers.conv2d(X_reshaped, filters=64, kernel_size=4, strides=2, padding=\"same\", activation=self.leaky_relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=2, padding=\"same\", activation=self.leaky_relu)\n",
    "\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=self.leaky_relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.contrib.layers.flatten(X) \n",
    "            mean = tf.layers.dense(X, units=self._number_of_latent_variables)\n",
    "            sd = 0.5 * tf.layers.dense(X, units=self._number_of_latent_variables)\n",
    "            epsilon = tf.random_normal([tf.shape(X)[0], self._number_of_latent_variables])\n",
    "            z = mean + tf.multiply(epsilon, tf.exp(sd)) #tf.multiply is elementwise or Hadamard multiplication!\n",
    "\n",
    "            return z, mean, sd\n",
    "\n",
    "    def _decoder(self, sampled_z):\n",
    "\n",
    "        with tf.variable_scope(\"decoder\", reuse=None):\n",
    "            X = tf.layers.dense(sampled_z, units=self._inputs_decoder, activation=self.leaky_relu)\n",
    "            X = tf.layers.dense(X, units=self._inputs_decoder, activation=self.leaky_relu)\n",
    "            X = tf.reshape(X, self._reshaped_dim)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=2, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X = tf.nn.dropout(X, self._keep_prob)\n",
    "            X = tf.layers.conv2d_transpose(X, filters=64, kernel_size=4, strides=1, \n",
    "                                 padding=\"same\", activation=tf.nn.relu)\n",
    "            X_flatten = tf.contrib.layers.flatten(X)\n",
    "            X_dense = tf.layers.dense(X_flatten, units=28*28, activation=tf.nn.sigmoid)\n",
    "            img = tf.reshape(X_dense, shape=[-1, 28, 28])\n",
    "\n",
    "            return img\n",
    "\n",
    "\n",
    "class AutoencoderTrainer(AutoencoderArchitecture):\n",
    "    \n",
    "    def __init__(self, batch_size=64, latent_variables=20, dec_in_channels=1, iterations=100):\n",
    "        super(AutoencoderTrainer, self).__init__(batch_size, latent_variables, dec_in_channels)\n",
    "\n",
    "        self._sampled, self._mean, self._standard = self._encoder()\n",
    "        self.__iterations = iterations\n",
    "        \n",
    "    @property\n",
    "    def iterations(self):\n",
    "        return self.__iterations\n",
    "    \n",
    "    @Decorators.timing\n",
    "    def train(self):\n",
    "\n",
    "        dec = self._decoder(self._sampled)\n",
    "        unreshaped = tf.reshape(dec, [-1, 28 * 28])\n",
    "        img_loss = tf.reduce_sum(tf.squared_difference(unreshaped, self._Y_flat), 1) #Log-likelihood \n",
    "        latent_loss = -0.5 * tf.reduce_sum(1.0 + 2.0 * self._standard - tf.square(self._mean) - tf.exp(2.0 * self._standard), 1) #Kullback-Leibler divergence\n",
    "        \n",
    "        loss = tf.reduce_mean(img_loss + latent_loss)\n",
    "        optimizer = tf.train.AdamOptimizer(0.0005).minimize(loss)\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer()) #Initializing variables\n",
    "        for i in range(self.__iterations):\n",
    "            batch = [np.reshape(b, [28, 28]) for b in self._mnist.train.next_batch(batch_size=self._batch_size)[0]]\n",
    "            labels = self._mnist.train.next_batch(batch_size=self._batch_size)[1]\n",
    "            sess.run(optimizer, feed_dict={self._X_in: batch, self._Y: batch, self._keep_prob: 0.8})\n",
    "        ModelAnalysis(sess, self._number_of_latent_variables, dec, self._sampled, self._keep_prob)._data_generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAnalysis(object):\n",
    "    \n",
    "    def __init__(self, sess, num_latent, decoder, sampled_distribution, keep_prob):\n",
    "        \n",
    "        self.__sess = sess\n",
    "        self.__num_of_latent_variables = num_latent\n",
    "        self.__decoder = decoder\n",
    "        self.__sampled_distribution = sampled_distribution\n",
    "        self.__keep_prob = keep_prob\n",
    "        self.__number_of_images = 5\n",
    "        \n",
    "        \n",
    "    def _data_generator(self):\n",
    "        \n",
    "        randoms = [np.random.normal(0, 1, self.__num_of_latent_variables) for _ in range(self.__number_of_images)]\n",
    "        imgs = self.__sess.run(self.__decoder, feed_dict={self.__sampled_distribution: randoms, self.__keep_prob: 1.0})\n",
    "        imgs = [np.reshape(imgs[i], [28, 28]) for i in range(len(imgs))]\n",
    "        for img in imgs:\n",
    "            plt.figure(figsize=(1,1))\n",
    "            plt.axis(\"off\")\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "            plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using tensorflow 1.14.0 for generating the variational autoencoder\n",
      "Extracting MNIST/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABJNJREFUeJzt2rmLVNsWx/FP285DK7bzPKEGRu0AGogjRioKgqAgxsYamBgZmxooRv4JgqCCCEYKjTjgBM6K7YC2Ew5t36DZdW7xeMm9Xavfea5vUlBVZ/fuH79a57fWPm39/f2S1jJsqDfwJ5AiB5AiB5AiB5AiB5AiB5AiB5AiBzA88o+1tbX933Y+/f39bf/ts3RyAClyAClyAClyAClyAClyAClyAClyAClyAKEdX6tpaxtousaMGePw4cNg1KhRTa8XL14Ely5dAj9+/Gj5vtLJAbRFHqS2anYxYsQIsGXLFnDs2DE3btwAr1+/bvrOrFmzwKRJk8DRo0fB7du3/9UecnYxxNTayePHjwdHjhwB+/btA52dne7du4fKsXfv3oXG+2vXrgUdHR1g9erV+Oc1Op08xNQyXbS3t4PNmzeDgwcPoqqrV69etXHjRvDq1Svw+/dvsGrVKrB06dKmtfbs2QPOnj076PtNJwdQSyePHTsW7NixA4wbNw5s27YN/Pr1y6dPn8DUqVPB9OnTweTJk8H3799ROf39+/eosvZg3qvSyQHU0sklMWzYsAF8+fIFVRd36tQp8+bNA7t378ZAF0iVl/v6+sCHDx/AzZs3MbgOLqSTA6iVk0u9XLNmDXj58iW4fv06qmTQ29tr5syZ4NmzZ2DZsmWoanK59sKFC+Ddu3ct23c6OYBaObnw+fNncOXKFfD48WPw6NEjMGHCBFu3bgXDhw/8i2UKd+bMGbBz505opJCSNlpBOjmAWjm51ORSP+fMmQMePHgAFi5cCI4fP+7Jkydg7ty54OPHj2DJkiVNawwbNuCzVs5waiVyEWLlypWohjml0di/fz9YvHhxo4T09vaCBQsWNH23NDBfv35t+b6zXARQKydPnDgRA20zVUu8YsUKVDe3W7duWbRoEZg/f37TtYWenh6wfPlyVGWjNCmDSTo5gFo5ubTTZWz57ds3MG3aNFRHSz09PR4+fIiq6Sht9ZQpU1ANjspgqHxe4uFgkk4OoBZOLtHt7du34Pnz56iOjs6fPw+6uroar5s2bUKVHt68eQONA9ay5p07d1D9KlpBOjmAWji55ONy5y+NRUkZxaWl8Whvb2+006WO//z5E9Vg/9y5c6C7u7tp7VaQTg6gFk4u9XP06NFg/fr10BjMb9++HZVr+/r63L9/H4wcORJVIilZugzpS/poJenkAGrh5MLs2bNRpYgyhyhHSCXrdnd3N2YVJT2Ua168eIEqZbQyVRTSyQHUwsklXZTMW2pveTSgDNzLeLOrq+s/OrrOzk5w4sQJcPny5aa1W0k6OYBaPXBYJmW7du0Chw4dQjVRK8f9HR0dnj59CtatWwcOHDgArl27hsF3cD5wOMTUysl/WwdVbS6Honv37gUzZsxw8uRJcPr0aVQdX6tIJw8xtXTy/yLp5CEmRQ4gRQ4gRQ4gRQ4gRQ4gRQ4gNCf/qaSTA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA0iRA/gLk0SBinxpanwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABHJJREFUeJzt27uPjWsUx/HPNkRchgxxj4QQRBSmUGBQ0NAIoXLpRCURpU6lUUhUkqm0KhGNP0DhPhJD3AtEgkTcx2XmFJNnP3GS0ziz15z3WN9usid7nvnlu9estZ53WiMjI5LOMmG8D/AnkCEHkCEHkCEHkCEHkCEHkCEHkCEHMDHyh7Varf/t5DMyMtL6p9fS5AAy5AAy5AAy5AAy5AAy5AAy5AAy5AAy5ABCJ74opkyZ4vjx4+DHjx9gyZIl4MKFC+DSpUvg+/fvHT9PmhxAK/IitVO7i4kTRz+QO3bsACdOnHD9+nXw9u1bMHPmTDBv3jwwf/58cOjQIXD37t1/dYbcXYwzjTZ52rRp4MiRI+Dw4cOgp6fHmzdvwKRJk8CjR4/AzZs3Ua1vtUYFXLt2Lfj27dtvnSVNHmca2V10dXWBzZs3g6NHj4Jbt26BoaEh69evB0+ePAFfv34FW7ZsAbNmzfrlvXbu3AnOnz8/5udNkwNopMlTp04F27dvBzNmzADbtm3DqJ1fvnwBy5cvh/bXc+fORa29pfsYGhrq2HnT5AAaaXJ3dzfYtWsXeP36NbR74/7+/nY/fPDgQVTbe3p6UM19/vw5uH37dsfOmyYH0CiTS0+7bt068OLFC9Tet7+/H7x//77dPTx79gysWbMGTJ8+HXz48AFcvnwZ9dPQCdLkABplcqHU0ytXroAHDx6gmt3d3W3//v2o9peafO7cObB7927w8eNH1D66E6TJATTK5GLlq1evwJw5c8DDhw/B4sWLwZkzZ9y5cwe1Ty5dxKJFi8C7d+9Q982dpFEhl2XWxo0bUVu5sig6efIkWLBggXv37qGGuWzZMtTlfVl9lnLRSbJcBNAok8sfr8+fP4OnT5+ijtdlET84ONguJWWFWcbpMl6XxVF5fcKEUd9+/vw55udOkwNolMllJC71tAwUxb4ygAwMDLTbuWJ9WSrNnj0bLFy4ENXsyZMn//L9Y0maHECjTC6j77Vr18Dq1atRh5MyUPT29rbrdOk8Sv0uI3gxe2Bg4Jf36ARpcgCNMnl4eBj1gZRSRwcHB1Gv+5cuXdruocv1UulMtm7dCi5evAiuXr2KznQVhTQ5gEaYXMbpcr2/YcMG1CmuXIKuWLECfPr0qX2pWjqOUnOL7WWp9PLly46fP00OoBEmF0pvu2rVKlSjy4MsZQl048YNvb29qLW3r68P3L9/H/W6qfTJnSRNDqARJpftW+mDV65ciVqri43lqqmvr6/9EGKxv0yJp0+fRr12iiBNDqBRDxyWTdmBAwfAsWPHUB8mLJu24eFhjx8/Bps2bQJ79+5FnfDG+vfOBw7HmUaZ/HfKVm7Pnj1g3759GL39OHXqFDh79iw6/28LafI402iT/0ukyeNMhhxAhhxAhhxAhhxAhhxAhhxAaJ/8p5ImB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB5AhB/AX5Wl4J6/yVR0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABH9JREFUeJzt3LuPjG0Yx/HPimPWeYeICBIkoqDYghBBaCQaBZ1IVKKiVqn9AQoVWhIRhVLrLA6hQIJ1SKxF4rCO8xabe+535X0TEXOth+vbPTPPzPPML79c87uu+57pabfbku4ybqxv4G8gRQ4gRQ4gRQ4gRQ4gRQ4gRQ4gRQ5gfOTFenp6/tjOp91u9/zfc+nkAFLkAFLkAFLkAFLkAFLkAFLkAFLkAFLkAEI7vm4zbtyIZ3p7ex06dAhMmDABTJkyBZw7dw6cPXsWDA8Pd/++un6FRE/kQmq3ZheTJ08G27ZtAwcOHPD48WPw9OlTVMcuXboULFy4EOzbtw9cu3YN/KweObsYYxrt5L6+PrB//35UJ/f19Xn16tWoc+/evYvq7A0bNoDPnz+DTZs2gffv3//UvaSTx5hGpouSGLZs2QL27NkDLly4AO7du2fNmjXg+vXrqHV77dq1YMaMGaiJZOfOneDYsWP4+dr8X6STA2ikk4sLN2/eDKZPnw62b98OhoaGfP36FSxbtgw6x7Nnzwbjx4989JJC3rx5A3p6RkprOrlhNMrJxWXfO/nly5fg9OnT4OTJk+bOnQt27NgB5s2bh+rgkiJKurhz5w5+rYML6eQAGunk/v5+VAffvHkTnDhxArx+/brj5EePHqG6v7zHt2/fwMWLF1Hzczq5oTTSySUpXLlyBdy4cQMj+RharZatW7ei5uCSQE6dOoVaq1+8eIGf7/R+hHRyAI1ycmFoaAgsWLAA3L59GyxevBgcPnzYpUuXRj02MDAAFi1aBJ4/f47q5FKju0EjRS6t8YcPH8C0adPAwYMHwdSpUzvnfvnyZdRjS5YsQS0jpfR0kywXATTKybNmzUKNbs+ePQMbN26ETmwbHBy0cuVKMH/+fNRm5O3bt6ht9PLly1GHTh8/fvzl951ODqARTi7RrbiyLIqWL6sSv8oQf2BgwIMHD1Db5Tlz5oCZM2eifgFOmjRp1HumkxtKo5xcWuSylPTp0yfUGl2OV6xYYf369agJpMS+MsQvDr58+TKyGWk8jXByGdqUGvzu3TvUseX9+/dRl/mHh4c7WwB6e3tHvWbdunXg/PnzqAOiMvLsBunkABrh5NKdTZw4EXU5vySGsqBaMm+73e600aXTK64v6eHWrVuodb6bWyPSyQE0wsmFUnNLN1fmEKV7e/jwIUYWR0vuLZtcVq1ahZpAyli0dIDdJJ0cQCOcXFJFmZiVjSqlzg4ODqJm4v7+/s70rXR+ZfnpyJEj4MyZM+huLS6kkwNohJOL28pwvmzg3rt3L2rGLbONJ0+edOrz6tWrwe7du8HVq1fR3SH996STA2jk1tmSm1utFti1axdqXm61Wo4fPw6OHj2K7v9sIbfOjjGNdPIPXCckNfybdPIY80eK/Lv9a+MfKfLvRoocQIocQIocQIocQGhO/ltJJweQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgeQIgfwD5wspFErzP2jAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABMFJREFUeJzt2tuLzX0Ux/HXZpzJOcaZSRSRouRCIldS4sYtV5Jyq+QfcOVPcIML+Qe4lSsllORQtJ1yluNgzHOxW/v37Hmepzya/Z35sd43c2p++zef3rP2Wuv3bQwODkq6y5iRvoE/gQy5ABlyATLkAmTIBciQC5AhFyBDLkBPyRdrNBq/7eQzODjY+K+fpckFyJALkCEXIEMuQIZcgAy5ABlyAYr2yaVoNBr/+jnEQ4qSDyvS5AL8FiaHrT09rT9nzpw5Tpw4AcaPHw9mzpwJLl68CM6fPw/evHnTca1uGJ4mF6BRsjYN9+5izJiWI9OmTQP79+8HR44cce/ePfD27VtUhs6bNw8sWLAAHDt2DFy6dAn8+PHjl+4ldxcjTC1NDoOXLFkC7fq7c+dOtMx+/vw5GDt2LHj69Cl48OAB2LRpU8fPN2/eDF6/fo3/X5vT5BGmVt1FdBGTJ08Ge/bsAfv27QPXrl0DX758sWHDBvDw4cOOa6xfvx5Mnz4dTJo0CRw4cACcOnUKfP/+fdjuO00uQC1MDoPj4+zZs8GWLVvA1KlTwbZt28DAwID+/n6wdOlStOyG+fPnd3z96tUr8OHDh67df5pcgFqYHO/00QmEjVu3bkXVEVy/fh2cPXvW3Llzwd69e1HV8bA+ePfuHbhx40bHaw0naXIBamFyEDU5anH0wrdv3wZnzpwBHz9+tHDhQvD48WOwevVqMGvWLPDs2TNUBkf/nCbXlFqYPLS7+Pr1K7h69SoqW+/fv49W97F9+3YwYcIEVLX4woULYPfu3eD9+/eotnFpck2phclBdBdPnjwBu3btQlVP16xZA44fP+7WrVugt7cX1eS3fPly8OLFC1RbuoGBAXTH5FqFHGvIGJljUTRjxgy0Vpy0RuZY4MdQEuFGaxflI8Id+phqOMlyUYBamByWLV68GHz79g3Vv3ysLRctWtT+fl9fH6olfQwwMT7Hm+XKlStRPaYK84eTNLkAo9rkoa1bmDrUuqi/U6ZMAc1m06NHj1C94cUQEivOMPzTp08d3w/Th/MNME0uwKg2OYgu4u7du6g6heDz588dH/v6+mzcuBHV4BLDRozgEydORDXARCvXlfvv2pWTNrUwOfrjqJNRR8eNG4fK8FgK9fT0tB8fxYozjI4HppcvXwZXrlxBtcTPsbqm1MLkGKfDyqi3y5YtAzt27EDreBYta2OVGfU8fjeuFaP4nTt38OuHWn6GNLkAtTA5+uRVq1ah2l3E8ifqbyzxm81m++hWs9kEa9eu7bjWzZs3wcuXL9Hdo7RpcgFGtclhV5g6dDqLCS+6i+iT161b1+6Do9bGMYJz586B06dPo7srziBNLkAtDhwOPZ516NAhcPjwYWgfk42D3v39/e1DK1GLjx49iuoQeGzyhos8cDjC1MLkocTWbcWKFeDgwYOojmn19va2a+/JkydR7Sa61Q+nySNMLU0eSkx1f3udf+w7uk2aPMKM6j75Z+nm3mE4SJMLkCEXIEMuQIZcgAy5AEX75D+VNLkAGXIBMuQCZMgFyJALkCEXIEMuQIZcgAy5ABlyATLkAmTIBciQC5AhFyBDLkCGXIAMuQAZcgEy5AJkyAXIkAuQIRcgQy7AX3KxuafLZFXvAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFkAAABYCAYAAACeV1sKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAABIBJREFUeJzt2klrVGkUh/FfORGMcYiCcUZQRBAVEYwiCCKuRUFw40LMTvwK+gncuha37ozDRnAjbhxwQHFAVJw1hKhx1vSieOt2NfSmO3XStz3PJpDKvfXm4V+nznnf2xgbG5N0lkkTvYDfgZQcQEoOICUHkJIDSMkBpOQAUnIAUyLfrNFo/G8nn7GxscbfvZZJDiAlB5CSA0jJAaTkAFJyACk5gJQcQEoOIHTi6zSNRnPo6u7udvToUTBpUjNHM2fOBOfPnwenT58GX79+7fi6MskBNCIPUju1dzFt2jSwd+9eMDAw4ObNm+Dbt2/lvcGSJUtAX18fOHToELhx48a/WkPuXUwwtU7ynDlzwOHDh8H+/fvBrFmzvH37FsyePRtcuXIFPH36FKxdu7btXjt27MA/r9GZ5Ammlt3FlCnNZe/cuRMcOHAAPHjwAAwPD7eS+vDhQzA6Ogo2bNiAqtsoyS31/OTJk+O+3kxyALVM8owZM8C2bdtQ1d0tW7aAoaEh3d3dqLqLefPmtf1t+X1XVxcYGRlB1YWM53dVJjmAWia5p6cH7Nq1C7x//x4MDg6Cc+fOWbRoEdizZw+aHQfVp6BQupA7d+5gfBNcyCQHUKskl3rZ398P7t69C968eYOqMxgZGWkl9/79+2D58uVg1apVqLqKs2fPglevXnVs3ZnkAGqZ5M+fP4N79+6BW7dugSdPnqBZf8v09+XLFzB//nxw6tQpsH37dlT1/NOnTx1bdyY5gFomeXh4GFWXUXrdZcuWgSNHjrh9+zaqGvz69WtUu2/l0zB16lR0pqso1EpyEbF582ZUo3Fvby+acmkOHGUYKWVg4cKFYMGCBW3Xfvz4EZ0ZQgpZLgKoVZLnzp2Lqv0qbdfWrVtRfbldvXq1tTlfElx+/vjxo+3axYsXozqm+vXr17ivO5McQC2SXOplSXLZ6iz1tnwRlk2g0dFRL168QDWoPHr0qO3alStXojq6mj59OqqWbjzJJAdQiyQXSjrLIWmpnx8+fEBVV/v7+1upL23e8+fPUY3i5ZqyqZ/DSM2pRZJL71qSWwaJktzr16+j6n37+vpaXcP379/b7rVu3Tpw8eJFcOHCBfDz589OLT+THEEtklwSWzqBcoBaRuRynF+muZ6eHpcvX267ptTcFStWgGvXroGXL1+is2N1JjmAWiS58Ncpbs2aNajq7tDQELh06ZLVq1ejOf3Bxo0bUU16z549Q2e7ikImOYBaJLnUy9JVlFpcavW7d+9QpXT9+vWtWrx06VJU26HHjx8HZ86cabt3J8kkB1CrBw4nT56M6rGsgYEBVBvyZV+iq6vL48ePwaZNm8DBgweh1XWM9/+dDxxOMLVK8p/ug2rXbd++fWD37t1o9svHjh0DJ06cQHWg2ikyyRNMLZP8XySTPMGk5ABScgApOYCUHEBKDiAlBxDaJ/+uZJIDSMkBpOQAUnIAKTmAlBxASg4gJQeQkgNIyQGk5ABScgApOYCUHEBKDiAlB5CSA0jJAaTkAFJyACk5gJQcQEoO4A++rH93+qga2gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time of method train: 5.499 seconds\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \n",
    "    print(\"Using tensorflow {} for generating the variational autoencoder\".format(tf.__version__))\n",
    "    tf.reset_default_graph()\n",
    "    trainer = AutoencoderTrainer(batch_size=64, latent_variables=49, iterations=10)\n",
    "    trainer.train()\n",
    "    \n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
